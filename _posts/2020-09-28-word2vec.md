---
layout: post
title: Word2vec
date : 28 sep 2020
category : ML
comments : true
---
# Word2vec

 : 본 포스팅은, 쉽게 씌어진 word2vec 블로그를 공부하며 정리하기 위해 작성되었습니다.

## 1. 단어 -> 숫자
#### 1) one hot encoding
 : 각 단어를 하나의 Feature로 활용해, 해당 단어가 속하는 Feature에만 1을 매기며, 나머지는 0으로 채우는 방법
  - 장점 : 표현이 쉬움.
  - 단점 : 단어와 단어간 관계가 전혀 드러나지 않음
    -  e.g. '강아지' & '멍멍이' 두 단어는 사실상 유사한 단어이나, 서로 다른 독립적인 feature로 인식됨

  <center>  
  <img src = '/assets/word2vec_OHE.png' width = '80%'>
  </center>

 => 단어와 단어간 관계를 벡터 공간상 위치로 표현하기 위한 시도 등장  => *'word embedding model


#### 2) Embedding test
 [한국어 Word2Vec](https://word2vec.kr/search/?query=%ED%95%9C%EA%B5%AD-%EC%84%9C%EC%9A%B8%2B%ED%8C%8C%EB%A6%AC)  

단어를 벡터간 위치값으로 변환시, 덧셈 & 뺄셈이 가능해짐. 위 사이트에서 단어와 단어를 빼거나 더했을 때 벡터간 위치의 변화로 인해 어떻게 의미가 변화하는지 경험해 볼 수 있음(e.g.  “한국 - 서울 + 도쿄 = 일본")


## 2. Sparse vs Dense Representatios
  * feature representation  : 대상을 표현 하는 방식으로, 자연어 처리의 경우 특정 '텍스트'를 판단하기 위해 해당 단어와 함께 등장하는 단어, 단어의 길이, 단어의 위치, 함께 쓰인 품사 등이 특정 '텍스트'를 표현하고 있는 언어적 정보가 될 수 있다. 이같은 속성에는 크게 Sparse representation과 dense representation으로 존재.  

#### 1) Sparse representation
 : one hot encoding or dummy variable,
  - 단어의 표현 : 표현해야 하는 단어의 수 N개일 때 -> N차원의 벡터 생성 후 해당 단어에만 1표시
  - 품사의 표현 : 모든 품사의 개수 만큼의 차원 생성 -> '명사'에 해당하는 벡터의 요소값만 1표시

  <img src = '/assets/word2vec_OHE_2.png' width = '45%'>
  <img src = '/assets/word2vec_OHE_3.png' width = '45%'>

#### 2) Dense representation

#### Refernce

[ [1] 쉽게 씌어진 word2vec](https://dreamgonfly.github.io/blog/word2vec-explained/#%EB%8B%A8%EC%96%B4-%EC%9E%84%EB%B2%A0%EB%94%A9word-embedding-%EB%A7%9B%EB%B3%B4%EA%B8%B0)  
